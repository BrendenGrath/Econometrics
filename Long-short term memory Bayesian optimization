One hidden layer long-short term memory (LSTM) neural network, with Bayesian optimization. Predictive analysis of forecasting accuracy for Swedish GDP growth with mixed frequencies.

####Set up the MIDAS model####
# Normalize the values between 0 and 1
normalize_minmax_neg1_1 <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Calculate log differences and multiply by 100
# Apply differencing and log transformation
quartely_gdp_diff <- diff(log(data1$quartely_gdp)) * 100
monthly_dax_diff <- diff(log(data$dax))
monthly_sp500_diff <- diff(log(data$sp500))
monthly_exrate_diff <- diff(data$exrate)
monthly_oilp_diff <- diff(log(data$oilp))
monthly_ppi_diff <- diff(log(data$ppi))
monthly_unemployment_diff <- diff(data$unemployment)
monthly_bond10y_diff <- diff(data$bond10y)
monthly_intrest3m_diff <- diff(data$intrest3m)
monthly_unemploymentg_diff <- diff(data$unemploymentg)
monthly_sekeur_diff <- diff(data$sek_eur)
monthly_sekusd_diff <- diff(data$sek_usd)

# Create a time series object with the correct start date (Q2 1997)
quartely_gdp <- ts(
  quartely_gdp_diff,
  start = c(1996, 2),  # Adjust start to Q2 1997
  end = c(2024, 3),
  frequency = 4
)

monthly_dax_diff_ts <- ts(monthly_dax_diff, 
                          start = c(1996, 4),  
                          end = c(2024, 9), 
                          frequency = 12)  
monthly_sp500_diff_ts <- ts(monthly_sp500_diff, 
                            start = c(1996, 4),  
                            end = c(2024, 9), 
                            frequency = 12) 
monthly_exrate_diff_ts <- ts(monthly_exrate_diff, 
                             start = c(1996, 4),  
                             end = c(2024, 9), 
                             frequency = 12)  
monthly_oilp_diff_ts <- ts(monthly_oilp_diff, 
                           start = c(1996, 4),  
                           end = c(2024, 9), 
                           frequency = 12) 
monthly_ppi_diff_ts <- ts(monthly_ppi_diff, 
                          start = c(1996, 4),  
                          end = c(2024, 9), 
                          frequency = 12) 
monthly_unemployment_diff_ts <- ts(monthly_unemployment_diff, 
                                   start = c(1996, 4),  
                                   end = c(2024, 9), 
                                   frequency = 12) 
monthly_bond10y_diff_ts <- ts(monthly_bond10y_diff, 
                              start = c(1996, 4),  
                              end = c(2024, 9), 
                              frequency = 12) 
monthly_intrest3m_diff_ts <- ts(monthly_intrest3m_diff, 
                                start = c(1996, 4),  
                                end = c(2024, 9), 
                                frequency = 12) 
monthly_unemploymentg_diff_ts <- ts(monthly_unemploymentg_diff, 
                                   start = c(1996, 4),  
                                   end = c(2024, 9), 
                                   frequency = 12) 
monthly_sekeur_diff_ts <- ts(monthly_sekeur_diff, 
                                    start = c(1996, 4),  
                                    end = c(2024, 9), 
                                    frequency = 12) 
monthly_sekusd_diff_ts <- ts(monthly_sekusd_diff, 
                             start = c(1996, 4),  
                             end = c(2024, 9), 
                             frequency = 12) 

# Normalize the min-max range for dax_diff if needed
monthly_dax_diff_normalized <- normalize_minmax_neg1_1(monthly_dax_diff_ts)
monthly_sp500_diff_normalized <- normalize_minmax_neg1_1(monthly_dax_diff_ts)
monthly_exrate_diff_normalized <- normalize_minmax_neg1_1(monthly_dax_diff_ts)
monthly_oilp_diff_normalized <- normalize_minmax_neg1_1(monthly_dax_diff_ts)
monthly_ppi_diff_normalized <- normalize_minmax_neg1_1(monthly_dax_diff_ts)
monthly_unemployment_diff_normalized <- normalize_minmax_neg1_1(monthly_unemployment_diff_ts)
monthly_bond10y_diff_normalized <- normalize_minmax_neg1_1(monthly_bond10y_diff_ts)
monthly_intrest3m_diff_normalized <- normalize_minmax_neg1_1(monthly_intrest3m_diff_ts)
monthly_unemploymentg_diff_normalized <- normalize_minmax_neg1_1(monthly_unemploymentg_diff_ts)
monthly_sekeur_diff_normalized <- normalize_minmax_neg1_1(monthly_sekeur_diff_ts)
monthly_sekusd_diff_normalized <- normalize_minmax_neg1_1(monthly_sekusd_diff_ts)

monthly_ind_prod <- ts(normalize_minmax_neg1_1(data$ind_prod), 
                       start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_man_prod <- ts(normalize_minmax_neg1_1(data$man_prod), 
                       start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_exp <- ts(normalize_minmax_neg1_1(data$exp), 
                  start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_imp <- ts(normalize_minmax_neg1_1(data$imp), 
                  start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_kpi <- ts(normalize_minmax_neg1_1(data$kpi), 
                  start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_car <- ts(normalize_minmax_neg1_1(data$car), 
                  start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_barnar <- ts(normalize_minmax_neg1_1(data$barnar), 
                     start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_barmar <- ts(normalize_minmax_neg1_1(data$barmar), 
                     start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_konhus <- ts(normalize_minmax_neg1_1(data$konhus), 
                     start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_eti <- ts(normalize_minmax_neg1_1(data$eti), 
                  start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_bartrade <- ts(normalize_minmax_neg1_1(data$bartrade), 
                       start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_barserv <- ts(normalize_minmax_neg1_1(data$barserv), 
                      start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_pmiman <- ts(normalize_minmax_neg1_1(data$pmiman), 
                     start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_pmiunp <- ts(normalize_minmax_neg1_1(data$pmiunp), 
                     start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_pmiprod <- ts(normalize_minmax_neg1_1(data$pmiprod), 
                      start = c(1996, 4), end = c(2024, 9), frequency = 12)
monthly_man_prodg <- ts(normalize_minmax_neg1_1(data$man_prodg), 
                      start = c(1996, 4), end = c(2024, 9), frequency = 12)

# Create high frequency data with all the variables
high_freq_data <- data.frame(
  barmar = monthly_barmar,  
  eti = monthly_eti,        
  barnar = monthly_barnar,  
  kpi = monthly_kpi,
  ind_prod = monthly_ind_prod,  
  man_prod = monthly_man_prod,
  unemployment = monthly_unemployment_diff_normalized,   
  exp = monthly_exp,  
  imp = monthly_imp,       
  ppi = monthly_ppi_diff_normalized,       
  car = monthly_car,       
  oilp = monthly_oilp_diff_normalized,      
  exrate = monthly_exrate_diff_normalized,
  intrest3m = monthly_intrest3m_diff_normalized,  
  bond10y = monthly_bond10y_diff_normalized, 
  sp500 = monthly_sp500_diff_normalized,     
  dax = monthly_dax_diff_normalized,       
  bartrade = monthly_bartrade,  
  barserv = monthly_barserv,  
  konhus = monthly_konhus,  
  pmiman = monthly_pmiman,    
  pmiunp = monthly_pmiunp,    
  pmiprod = monthly_pmiprod,
  man_prodg = monthly_man_prodg,
  unemploymentg = monthly_unemploymentg_diff_normalized,
  sekeur = monthly_sekeur_diff_normalized,    
  sekusd = monthly_sekusd_diff_normalized
)

# Create low-frequency data.frame for GDP (already quarterly)
low_freq_data <- list(
  y = quartely_gdp,  # Quarterly GDP (log-differenced)
  trend = 1:length(quartely_gdp)  # Time trend variable for the GDP
)

# MIDAS regression
midas_model_full <- midas_u(
  y ~ 
    mls(barmar, 0, 3) +    # Lag structure for barmar (monthly to quarterly)
    mls(eti, 0, 3) +       # Lag structure for eti (monthly to quarterly)
    mls(barnar, 0, 3) +    # Lag structure for barnar (monthly to quarterly)
    mls(ind_prod, 0, 3) +       # Lag structure for ind_prod (monthly to quarterly)
    mls(man_prod, 0, 3) +    # Lag structure for man_prod (monthly to quarterly)
    mls(unemployment, 0, 3) +    # Lag structure for man_prod (monthly to quarterly)
    mls(exp, 0, 3) +       # Lag structure for exp (monthly to quarterly)
    mls(imp, 0, 3) +    # Lag structure for imp (monthly to quarterly)
    mls(kpi, 0, 3) +       # Lag structure for kpi (monthly to quarterly)
    mls(ppi, 0, 3) +       # Lag structure for ppi (monthly to quarterly)
    mls(car, 0, 3) +       # Lag structure for car (monthly to quarterly)
    mls(oilp, 0, 3) +      # Lag structure for oilp (monthly to quarterly)
    mls(exrate, 0, 3) +    # Lag structure for bond1m (monthly to quarterly)
    mls(intrest3m, 0, 3) +       # Lag structure for interest rate 3m (monthly to quarterly)
    mls(bond10y, 0, 3) +    # Lag structure for bond10y (monthly to quarterly)
    mls(sp500, 0, 3) +     # Lag structure for sp500 (monthly to quarterly)
    mls(dax, 0, 3) +       # Lag structure for dax (monthly to quarterly)
    mls(bartrade, 0, 3) +  # Lag structure for bartrade (monthly to quarterly)
    mls(barserv, 0, 3) +   # Lag structure for barserv (monthly to quarterly)
    mls(konhus, 0, 3) +   # Lag structure for konhus (monthly to quarterly)
    mls(pmiman, 0, 3) +    # Lag structure for pmiman (monthly to quarterly)
    mls(pmiunp, 0, 3) +    # Lag structure for pmiunp (monthly to quarterly)
    mls(pmiprod, 0, 3) +   # Lag structure for pmiprod (monthly to quarterly)
    mls(man_prodg, 0, 3) +    # Lag structure for pmiunp (monthly to quarterly)
    mls(unemploymentg, 0, 3) +   # Lag structure for pmiprod (monthly to quarterly)
    mls(sekeur, 0, 3) +    # Lag structure for pmiunp (monthly to quarterly)
    mls(sekusd, 0, 3) +   # Lag structure for pmiprod (monthly to quarterly)
    trend,                 # Optional trend component
  data = c(low_freq_data, high_freq_data)
)

# Summary of the model
summary(midas_model_full)

# Convert midas_full model to data frame
midas_full_df <- data.frame(
  y = midas_model_full$model,
  fitted = midas_model_full$fitted.values,
  residuals = midas_model_full$residuals
)

#Remove the last three rows of irrelevant variables
midas_full_df <- midas_full_df %>%
  select(-c(tail(names(midas_full_df), 3)))

# Columns to convert
col_midas_full_df <- 1:28  
ts_names <- c("GDP", "BARMAR", "ETI", "BARNAR", "IND_PROD", "MAN_PROD",
              "UNEMPLOYMENT","EXP", "IMP","KPI", "PPI", "CAR", "OILP", "EXRATE",
              "INTREST3M", "BOND10Y", "SP500", "DAX", "BARTRADE", "BARSERV",
              "KONHUS","PMIMAN", "PMIUNP", "PMIPROD", "MAN_PRODG", "UNEMPLOYMENTG", "SEKEUR", "SEKUSD")

# ConGDP# Convert columns to time series
ts_list <- lapply(col_midas_full_df, function(i) {
  ts(as.numeric(midas_full_df[[i]]), start = c(1996, 2), frequency = 4)
})

# Name the time series in the list
names(ts_list) <- ts_names

# To show each TS individually in the global environment:
list2env(ts_list, envir = .GlobalEnv)

# Combine into a dataset and plot
midas_full_df <- cbind(GDP, BARMAR, ETI, BARNAR, IND_PROD, MAN_PROD, UNEMPLOYMENT,
                       EXP, IMP, KPI, PPI, CAR, OILP, EXRATE, INTREST3M, BOND10Y, SP500, DAX, BARTRADE,
                       BARSERV, KONHUS, PMIMAN, PMIUNP, PMIPROD, MAN_PRODG, UNEMPLOYMENTG, SEKEUR, SEKUSD)

# Move the first column (GDP/target) to the last position,
# to match with the code below to predict the correct variable
midas_full_df <- midas_full_df[, c(2:ncol(midas_full_df), 1)]

#Plot the Series
autoplot(cbind(GDP, BARMAR, ETI, BARNAR, IND_PROD, MAN_PROD, UNEMPLOYMENT, EXP, IMP, KPI, PPI,
               CAR, OILP, EXRATE, INTREST3M, BOND10Y, SP500, DAX,
               BARTRADE, BARSERV, PMIMAN, PMIUNP, PMIPROD, MAN_PRODG, UNEMPLOYMENTG))

# Plot GDP
plot(GDP, type = "l", col = "blue", lwd = 2,
     main = "",
     ylab = "Quartely Growth Rate (%)",
     xlab = "")
grid()

####AR(1) bench mark model####
set.seed(123)
midas_full_df <- as.data.frame(midas_full_df)  # Convert to a data frame

#ADF test, check that the variables are stationary
# Create an empty list to store results:
result_adf <- list()

# Loop through the dataset: perform the test, retrieve test statistics and 
# 5% critical value
for (i in colnames(midas_full_df)){
  adf_result <- ur.df(midas_full_df [, i], type = "drift", selectlags = "AIC")
  tau2_stat <- adf_result@teststat[1]
  tau2_crit_val <- adf_result@cval[1, 2]
  phi1_stat <- adf_result@teststat[2]
  phi1_crit_val <- adf_result@cval[2, 2]
  result_adf[[i]] <- list(
    tau2_stat = tau2_stat,
    tau2_crit_val = tau2_crit_val,
    phi1_stat = phi1_stat,
    phi1_crit_val = phi1_crit_val
  )
}

rm(list=c("adf_result", "i", "tau2_stat", "tau2_crit_val", 
          "phi1_stat", "phi1_crit_val"))

# Print the results:
for (i in names(result_adf)) {
  cat("Series:", i, "\n")
  cat("tau2:", result_adf[[i]]$tau2_stat, "\n")
  cat("5% Critical Value:", result_adf[[i]]$tau2_crit_val, "\n\n")
  cat("phi1:", result_adf[[i]]$phi1_stat, "\n")
  cat("5% Critical Value:", result_adf[[i]]$phi1_crit_val, "\n")
  cat("########", "\n\n")
}

# Define the time series for GDP
midas_full_df$GDP <- ts(midas_full_df$GDP, frequency = 4)

Yt <- midas_full_df$GDP  # GDP column

# Specify the number of observations to be used for testing (23)
test_size <- 34

# Calculate the split index
train_size <- nrow(midas_full_df) - test_size

# Split data into training and testing sets
YT <- Yt[1:train_size]    # First portion for training (up to the train_size index)
Ytt <- Yt[(train_size + 1):nrow(midas_full_df)]    # Last 23 observations for testing

# Fit the AR(1) model on the training data
gdp_ar1 <- Arima(YT, order = c(1, 0, 0), include.drift = TRUE)
print(gdp_ar1)

# Forecast for the testing period (next 23 periods)
forecasts <- forecast(gdp_ar1, h = length(Ytt))

# Extract forecast values and confidence intervals
forecast_values <- data.frame(
  Quarter = time(forecasts$mean),
  Actual = as.numeric(Ytt),  # Actual values for comparison
  Forecast = as.numeric(forecasts$mean),
  Lower_80 = as.numeric(forecasts$lower[, 1]),
  Upper_80 = as.numeric(forecasts$upper[, 1]),
  Lower_95 = as.numeric(forecasts$lower[, 2]),
  Upper_95 = as.numeric(forecasts$upper[, 2])
)

# Print the forecasted values and confidence intervals
print(forecast_values)

####Neural network with Bayesian Optimization####
# 1. Förbered data
set.seed(123)
X <- midas_full_df[, -ncol(midas_full_df)]
Y <- midas_full_df[, ncol(midas_full_df)]

train_indices <- 1:floor(0.7 * nrow(X))  # 70% träning
X_train <- X[train_indices, ]
Y_train <- Y[train_indices]
X_val <- X[(max(train_indices) + 1):nrow(X), ]
Y_val <- Y[(max(train_indices) + 1):nrow(X)]

# Konvertera till rätt format
X_train <- as.matrix(X_train)
X_val <- as.matrix(X_val)
Y_train <- as.numeric(Y_train)
Y_val <- as.numeric(Y_val)

# 2. Definiera parametrar för LSTM
timesteps <- 3  # Tidssteg
n_features <- ncol(X)  # Antal features

# Omforma data för LSTM (samples, timesteps, features)
reshape_data <- function(X, timesteps) {
  n_samples <- nrow(X) - timesteps + 1
  array_X <- array(0, dim = c(n_samples, timesteps, ncol(X)))
  
  for (i in 1:n_samples) {
    array_X[i, , ] <- X[i:(i + timesteps - 1), ]
  }
  
  return(array_X)
}

X_train <- reshape_data(X_train, timesteps)
X_val <- reshape_data(X_val, timesteps)

# Justera Y så att den matchar tidsstegen
Y_train <- Y_train[(timesteps):length(Y_train)]
Y_val <- Y_val[(timesteps):length(Y_val)]

# 3. Skapa LSTM-modellen
build_model <- function(units1, dropout_rate, learning_rate) {
  model <- keras_model_sequential() %>%
    layer_lstm(units = round(units1), input_shape = c(timesteps, n_features), return_sequences = FALSE) %>%
    layer_dropout(rate = dropout_rate) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    optimizer = optimizer_adam(learning_rate = learning_rate),
    loss = 'mean_squared_error',
    metrics = c('mean_absolute_error')
  )
  
  return(model)
}

# 4. Träna modellen
train_model <- function(units1, epochs, batch_size, dropout_rate, learning_rate) {
  model <- build_model(units1, dropout_rate, learning_rate)
  
  history <- model %>% fit(
    X_train, Y_train,
    epochs = round(epochs),
    batch_size = round(batch_size),
    validation_data = list(X_val, Y_val),
    verbose = 0
  )
  
  val_loss <- min(history$metrics$val_loss)
  return(-val_loss)  # Returnerar negativt värde för optimering
}

# 5. Bayesian Optimization
objective_function <- function(units1, epochs, batch_size, dropout_rate, learning_rate) {
  result <- train_model(
    units1 = units1,
    epochs = epochs,
    batch_size = batch_size,
    dropout_rate = dropout_rate,
    learning_rate = learning_rate
  )
  return(list(Score = result))
}

bounds <- list(
  units1 = c(8, 160),
  epochs = c(10, 100),
  batch_size = c(8, 128),
  dropout_rate = c(0.1, 0.5),
  learning_rate = c(1e-5, 1e-2)
)

set.seed(123)
opt_results <- bayesOpt(
  FUN = objective_function,
  bounds = bounds,
  initPoints = 10,
  iters.n = 2,
  acq = "ei"
)

best_params <- getBestPars(opt_results)
print(best_params)

####Find the optimal lag selection with cross-validation for neural network####
# Define the function to generate lagged features
add_lags <- function(data, lags) {
  lagged_data <- as.data.frame(data)
  for (lag in lags) {
    lagged_data <- lagged_data %>%
      mutate(across(everything(), ~ dplyr::lag(., lag), .names = "{col}_Lag{lag}"))
  }
  return(lagged_data)
}

# Cross-validation with BIC-based lag selection
select_bic_lag_cv <- function(X, Y, max_lag, k_folds = 5) {
  bic_values <- numeric(max_lag)
  
  for (lag in 0:max_lag) {
    fold_bic <- numeric(k_folds)
    
    # Add lags
    X_lagged <- add_lags(X, 0:lag)
    X_lagged <- na.omit(X_lagged)  # Remove NA rows caused by lagging
    
    # Align Y to X_lagged without truncating final observations
    Y_lagged <- Y[lag:length(Y)]
    Y_lagged <- Y_lagged[(length(Y_lagged) - nrow(X_lagged) + 1):length(Y_lagged)]
    
    # Confirm proper alignment
    stopifnot(nrow(X_lagged) == length(Y_lagged))
    
    # Cross-validation
    n <- nrow(X_lagged)
    fold_size <- floor(n / k_folds)
    set.seed(123)
    indices <- sample(1:n)
    
    for (fold in 1:k_folds) {
      # Split data into train and validation sets
      fold_start <- (fold - 1) * fold_size + 1
      fold_end <- ifelse(fold == k_folds, n, fold * fold_size)
      
      val_indices <- indices[fold_start:fold_end]
      train_indices <- setdiff(indices, val_indices)
      
      X_train <- X_lagged[train_indices, , drop = FALSE]
      Y_train <- Y_lagged[train_indices]
      X_val <- X_lagged[val_indices, , drop = FALSE]
      Y_val <- Y_lagged[val_indices]
      
      # Fit linear model and calculate BIC
      model <- lm(Y_train ~ ., data = as.data.frame(X_train))
      bic <- BIC(model)
      fold_bic[fold] <- bic
    }
    # Average BIC across folds
    bic_values[lag] <- mean(fold_bic)
  }
  
  optimal_lag <- which.min(bic_values)
  return(list(optimal_lag = optimal_lag, bic_values = bic_values))
}

# Apply cross-validation for lag selection
max_lag <- 8  # Maximum lag to test
k_folds <- 5   # Number of folds for cross-validation

cv_result <- select_bic_lag_cv(X, Y, max_lag, k_folds)
cat("Optimal Lag Selected by BIC with Cross-Validation:", cv_result$optimal_lag, "\n")

####Feed forward neural network final model####
# Function to add lags to features
add_lags <- function(data, num_lags) {
  lagged_data <- as.data.frame(data)
  for (lag in 0:2) {
    lagged_data <- lagged_data %>%
      mutate(across(everything(), ~ dplyr::lag(., lag), .names = "{col}_Lag{lag}"))
  }
  return(na.omit(lagged_data))  # Remove NA rows
}

# Function to scale features (Z-score normalization)
scale_features <- function(data) {
  scaled_data <- as.data.frame(scale(data))
  return(scaled_data)
}

# Function to preprocess data: lagging, scaling, reshaping
preprocess_data <- function(X, Y, timesteps) {
  X_lagged <- add_lags(X, timesteps)  # Apply lags
  X_scaled <- scale_features(X_lagged)  # Scale features
  
  # Align target variable Y with lagged features
  Y_lagged <- Y[(timesteps + 1):length(Y)]
  Y_lagged <- Y_lagged[(length(Y_lagged) - nrow(X_scaled) + 1):length(Y_lagged)]
  
  # Ensure data alignment
  stopifnot(nrow(X_scaled) == length(Y_lagged))
  
  # Convert to matrix format
  X_matrix <- as.matrix(X_scaled)
  Y_matrix <- as.numeric(Y_lagged)
  
  # Reshape for LSTM (samples, timesteps, features)
  n_samples <- nrow(X_matrix)
  n_features <- ncol(X_scaled) / timesteps  # Features per time step
  X_reshaped <- array(X_matrix, dim = c(n_samples, timesteps, n_features))
  
  return(list(X = X_reshaped, Y = Y_matrix))
}

# Set parameters
set.seed(123)
timesteps <- 3  # Number of time steps for LSTM
train_ratio <- 0.7  # Train-validation split

# Preprocess data
data_processed <- preprocess_data(X, Y, timesteps)
X_full <- data_processed$X
Y_full <- data_processed$Y

# Split into training and validation sets
train_size <- floor(train_ratio * length(Y_full))
X_train <- X_full[1:train_size, , ]
Y_train <- Y_full[1:train_size]
X_val <- X_full[(train_size + 1):dim(X_full)[1], , ]
Y_val <- Y_full[(train_size + 1):length(Y_full)]

# Define LSTM model
build_lstm_model <- function(input_shape) {
  model <- keras_model_sequential() %>%
    layer_lstm(units = 14.51, input_shape = input_shape, return_sequences = FALSE) %>%
    layer_dropout(rate = 0.41) %>%
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    optimizer = optimizer_adam(learning_rate = 0.0008),
    loss = 'mean_squared_error',
    metrics = c('mean_absolute_error')
  )
  
  return(model)
}

# Build and summarize model
lstm_model <- build_lstm_model(c(timesteps, dim(X_train)[3]))
summary(lstm_model)

# Train model
history <- lstm_model %>% fit(
  X_train, Y_train,
  epochs = 87.96,
  batch_size = 34.66,
  validation_data = list(X_val, Y_val),
  verbose = 2
)

# Plot training history
plot(history)

# Evaluate model
evaluation <- lstm_model %>% evaluate(X_val, Y_val)
cat("Validation Loss:", evaluation$loss, "\n")
cat("Validation Mean Absolute Error:", evaluation$mean_absolute_error, "\n")

# Make predictions
predictions <- lstm_model %>% predict(X_val)

# Plot actual vs predicted
results_df <- data.frame(
  Index = seq_along(Y_val),
  Actual = Y_val,
  Predicted = as.numeric(predictions)
)

ggplot(results_df, aes(x = Index)) +
  geom_line(aes(y = Actual, color = "Actual"), linewidth = 1) +
  geom_line(aes(y = Predicted, color = "Predicted"), linewidth = 1, linetype = "solid") +
  labs(
    title = "",
    x = "Quarters",
    y = "Quartely Growth Rate (%) ",
    color = "Legend"
  ) +
  theme_minimal()

####RMSE and MAE calculations####
# Neural Network and AR(1) Forecast Comparison with RMSE and MAE by Horizon

# Neural Network predictions (on test set)
forecast_nn <- predictions  

# AR(1) model predictions
forecast_ar <- forecast_values$Forecast  

# Actual GDP values (test set alignment)
actual <- Ytt  

# Calculate forecast errors for Neural Network and AR(1)
error_nn <- actual - forecast_nn
error_ar <- actual - forecast_ar

# Calculate overall RMSE for Neural Network and AR(1)
rmse_nn <- sqrt(mean((actual - forecast_nn)^2, na.rm = TRUE))
rmse_ar1 <- sqrt(mean((forecast_values$Actual - forecast_values$Forecast)^2, na.rm = TRUE))

cat("Overall RMSE for Neural Network Model:", rmse_nn, "\n")
cat("Overall RMSE for AR(1) Model:", rmse_ar1, "\n")

# Calculate overall MAE for Neural Network and AR(1)
mae_nn <- mean(abs(actual - forecast_nn), na.rm = TRUE)
mae_ar <- mean(abs(actual - forecast_ar), na.rm = TRUE)

cat("Overall MAE for Neural Network Model:", mae_nn, "\n")
cat("Overall MAE for AR(1) Model:", mae_ar, "\n")

# Function to calculate RMSE and MAE
evaluate_forecast <- function(actual, predicted) {
  list(
    RMSE = sqrt(mean((actual - predicted)^2, na.rm = TRUE)),
    MAE = mean(abs(actual - predicted), na.rm = TRUE)
  )
}

# Define forecast horizons
horizons <- c(1, 3, 6, 9, 12)

# Initialize empty lists for evaluation results
nn_results <- list()
ar1_results <- list()

# Loop through each horizon
for (h in horizons) {
  # Neural Network: Align actual and predicted values
  actual_h_nn <- actual[(h + 1):length(actual)]
  predicted_h_nn <- forecast_nn[1:(length(forecast_nn) - h)]
  
  # AR(1): Align actual and predicted values
  actual_h_ar <- forecast_values$Actual[(h + 1):nrow(forecast_values)]
  predicted_h_ar <- forecast_values$Forecast[1:(nrow(forecast_values) - h)]
  
  # Evaluate Neural Network and AR(1) forecasts
  nn_results[[paste0("Horizon_", h)]] <- evaluate_forecast(actual_h_nn, predicted_h_nn)
  ar1_results[[paste0("Horizon_", h)]] <- evaluate_forecast(actual_h_ar, predicted_h_ar)
}

# Print evaluation results for each horizon
cat("\nNeural Network Evaluation by Horizon:\n")
print(nn_results)

cat("\nAR(1) Evaluation by Horizon:\n")
print(ar1_results)

# Summary of Results
cat("\nSummary of Forecast Evaluation:\n")
cat("Overall RMSE and MAE:\n")
cat("Neural Network: RMSE =", rmse_nn, ", MAE =", mae_nn, "\n")
cat("AR(1): RMSE =", rmse_ar1, ", MAE =", mae_ar, "\n")

####Diebold-mariano test####
# RMSE calculation
#Forecast error significance test Diebold Mariano Feed forward NN and AR(1)
horizons <- c(1, 3, 6, 9, 12)  # Horizons you're testing
powers <- c(2, 1)  # 2 for RMSE, 1 for MAE

# Initialize a list to store results
dm_results <- list()

# Loop through powers and horizons to compute DM tests
for (power in powers) {
  metric <- ifelse(power == 2, "RMSE", "MAE")  # Determine metric name
  
  for (h in horizons) {
    # Perform the DM test with the variance estimator adjustment
    test_result <- tryCatch({
      dm.test(error_nn, error_ar, h = h, power = power, varestimator = "bartlett")
    }, warning = function(w) {
      cat("Warning for horizon", h, ":", w$message, "\n")
      return(NULL)  # If there's a warning, return NULL (no result)
    }, error = function(e) {
      cat("Error for horizon", h, ":", e$message, "\n")
      return(NULL)  # If there's an error, return NULL (no result)
    })
    
    # Only store result if test was successful
    if (!is.null(test_result)) {
      dm_results[[paste0(metric, "_Horizon_", h)]] <- test_result
    }
  }
}

# Print the results
print(dm_results)
